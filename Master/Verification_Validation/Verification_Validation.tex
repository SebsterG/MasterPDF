\chapter{Verification and validation. }
When we set out to solve a real world problem with numerical computing, we start by defining the mathematics, we implement the equations numerically and solve them on a computer. We then use the solutions to extract data that will answer the questions we set out to solve. A problem then immediately arises, is this solution correct? To answer this we need to answer another question, are the equations solved correct numerically, if so is the problem defined correct mathematically in accordance with the governing laws and equations?
Without answering these questions, being confident that your solutions are correct is difficult \cite{Selin2014}. The goal of this section will hence be to verify and validate the different numerical schemes. \\
We start with Verification, which is the process of assessing numerical correctness and accuracy of a computed solution. Then comes Validation, which is assessing physical accuracy of the numerical model, a process which is done by comparing numerical simulation with experimental data. In simple terms we check that we are solving the equations right and then that we are solving the right equations. The process of Verification has to always come before Validation. Because there is no need in checking if we are using the right equations if the equations are not solved right. 

\section*{Verification}
In verification we get evidence that the numerical model derived from mathematics is solved correctly by the computer. The strategy will be to identify, quantify and reduce errors cause by mapping a mathematical model to a computational model. This does not address wether or not the mathematical model is in alignment with the real world only that our model is computed correctly. To verify that we are computing correctly we can compare our computed solution to an exact solution. But the problem is that there are no known exact solution to for instance the Navier-Stokes equations, other than for very simplified problems.
In tackling these problems there are multiple classes of test that can be performed, and the most rigorous is the \textit{Method of manufactured solution} \cite{Oberkampf2010}. Rather than looking for an exact solution we manufacture one. The idea is to make a solution \textit{a priori},  and use this solution to generate an analytical source term for the governing PDEs and than run the PDE with the source term to get a solution hopefully matching the manufactured one. The manufactured solution does not need to have a physically realistic relation, since the solution deals only with the mathematics. 
The procedure is as follows \cite{Oberkampf2010}:
\begin{itemize}
\item We define a mathematical model on the form $ L(u) = 0$ where $L(u)$ is a differential operator and $u$ is a dependent variable.
\item Define the analytical form of the manufactured solution $\hat{u}$
\item Use the model $L(u)$ with $\hat{u}$ inserted to obtain an analytical source term $ f = L(\hat{u}) $
\item Initial and boundary conditions are enforced from $\hat{u}$
\item Then use this source term to calculate the solution $u$, $L(u) = f $
\end{itemize}

After the solution has been computed we perform systematic convergence tests \cite{Roache}. The idea of order of convergence test is based on the behavior of the error between the manufactured exact solution and the computed solution. When we increase the number of spatial points ($ \Delta x, \Delta y$ or $ \Delta z$) or decrease timestep($\Delta t$), we expect the error to get smaller. Its the rate of this error that lets us now wether the solution is converging correctly.
If we let $u$ be the numerical solution and $ u_e $ be the exact solution, $|| . ||$ be the $L^2$ norm, we define the error as:
\begin{equation}
E = || u - u_e ||
\end{equation}
If we assume that the number of spatial points are equal in all directions the error is expressed as
\begin{equation}
\label{eq:Error}
 E = C_1 \Delta x^k+ C_2 \Delta t^l 
\end{equation}
where $ k = m+1 $ and m is the polynomial degree of the spatial elements. 
If we for instance reduce $\Delta t$ significantly than $\Delta x$ will dominate, and $\Delta t$ is negligible . If we then look at the errors in two timesteps, using \eqref{eq:Error}:
\begin{align}
\frac{E_{n+1}}{E_n} = \big( \frac{\Delta x_n+1}{\Delta x_n} \big)^k \\
k = \frac{log( \frac{E_{n+1}}{E_n}) }{ log(\frac{\Delta x_n+1}{\Delta x_n})}
\end{align}
We can use this to find the observed order of convergence and match with the theoretical for given 

\input{./Verification_Validation/MMS_FSI/MMS_FSI}

\section*{Validation}
After the code has been verified to see that we are indeed computing in the right fashion. We move on to Validation which is the process of determining if the model gives an accurate representation of the real world within the bounds of the intended use \cite{Selin2014}. A model is made for a specific purpose, its only valid in respect to that purpose \cite{Macal2005}. If the purpose is complex and trying to answer multiple question then the validity need to be determined to each question. The idea is to validate the solver \textsl{brick by brick}. We start with simple testing of each part of the model and build more complexity and eventually testing the whole model.Three issues have been identified in this process \cite{Selin2014}: Quantifying the accuracy of the model by comparing responses with experimental responses, interpolation of the model to conditions corresponding to the intended use and determining the accuracy of the model for the conditions under which its meant to be used.  For example if our solver needs to model fluid which is turbulent we have to validate our model to catch these turbulences and as we shall see later the Taylor-Green benchmark is a good test. Well known benchmarks will be used as validation, we will see in this chapter that these tests supply us with a problem setup, initial and boundary conditions, and lastly results that we can compare with. The process of Validation is also, as I have experienced, a way to figure out at what size timestep and number of spatial points the model can handle to run. As we will see in the chapter all the benchmarks are run with different timesteps and number of cells to see how it reacts. The problem with using benchmarks with known data for comparison is that we do not test the model blindly. It is easier to mold the model to the data we already have. As Oberkampf and Trucano in \cite{Selin2014} puts it ``Knowing the ``correct" answer beforehand is extremely seductive, even to a saint.''. Knowing the limitations of our tests will therefore strengthen our confidence in the model. It really can be an endless process of verifying and validating if one does not clearly now the bounds of sufficient accuracy. 

\cite{Selin2014} \\
In the following we will look at tests for the fluid solvers both alone, testing laminar to turbulent flow, and with solid. We will test the solid solver, and lastly the entire coupled FSI problem. 

\input{./Verification_Validation/Hron_Turek/Hron_Turek}

%\input{./Verification_Validation/Flexible_tube/Flexible_tube}

\input{./Verification_Validation/Mesh_motion_results/Mesh_motion_results}
\input{./Verification_Validation/Temporal_stability/Temporal_stability}










