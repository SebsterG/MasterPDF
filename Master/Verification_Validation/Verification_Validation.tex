\chapter{Verification and validation. }
When we set out to solve a real world problem with numerical computing, we start by defining the mathematics, we implement the equations numerically and solve on a computer. We then use the solutions to extract data that will answer the questions of the problem we set out so solve. A problem then immediately arises, is this solution correct? To answer this we need to answer another question, is the problem defined correct mathematically, and if so are these equations solved correct numerically? Without answering these questions, being confident that your solutions are correct is difficult. \cite{Selin2014} The goal of this section will hence be to verify and validate the different numerical schemes. \\
We start with Verification, which is the process of assessing numerical correctness and accuracy of a computed solution. Then comes Validation, which is assessing physical accuracy of the numerical model, a process which is done by comparing numerical simulation with experimental data or so called benchmark tests.


\section{Verification}
In verification we get evidence that the numerical model derived from mathematics is solved correctly by the computer. The strategy will be to identify, quantify and reduce errors cause by mapping a mathematical model to a computational model. This does not address wether or not the mathematical model is in alignment with the real world only that our model is computed correctly.
In verifying the code, order of convergence tests will be the most rigorous. To do this test we will use the method of manufactured solutions (MMS) \cite{Roache2002}. This method entails manufacturing an exact solution that is non trivial but analytic. The solution defines the boundary conditions and is passed through the equations giving a source term, named $f$. This source term is set to equalize the given equation, and then a solution is calculated. If the calculation is correct our calculated solution should equal the manufactured solution down to a give precision, computers are only precise to about $10^-16$. We can then increase for instance the number of cells in our computational domain, and see if the difference between the manufactured and computed solution (eg. error) gets smaller. The rate at which the error reduces can be checked with mathematical theory, we can than be more confident that our computation is correct. This will also be done in time, by reducing the time steps and investigating the error. The manufactured solution does not have to have any physical relation, and this fact does not implicate a less accurate verification. The solution needs only be non-trivial.

After the solution has been computed we perform systematic convergence tests \cite{Roache}. The idea of order of convergence test is based on the behavior of the error between the manufactured exact solution and the computed solution. When we increase the number of spatial points or decrease timestep, we expect the error to get smaller. Its the rate of this error that lets us now wether the solution is converging and hence that we are computing in the right fashion.
If we assume that the number of spatial points are equal in all directions we know that the error behaves like
$$ E = C_1 \Delta x^k+ C_2 \Delta t^l $$
where $ k = m+1 $ and m is the polynomial degree of the spatial elements. This means that when we compute with Taylor-Hood elements (P2-P1) we should expect to get a convergence rate of 2 in space and 1 in time.\
The convergence rates are computed as:
\begin{align}
\frac{E_{n+1}}{E_n} = \big( \frac{\Delta x_n+1}{\Delta x_n} \big)^k \\
k = \frac{log( \frac{E_{n+1}}{E_n}) }{ log(\frac{\Delta x_n+1}{\Delta x_n})}
\end{align}


\input{./Verification_Validation/MMS_FSI/MMS_FSI}


\section{Validation}
After the code has been verified to see that we are indeed computing in the right fashion. We have to see that it is the right equations that are being solved. This is achieved using known benchmark tests. These tests supply us with a problem setup, initial and boundary conditions, and lastly results that we can compare with. We can then determine the accuracy of the computational model, and see if these meet the requirement needed to solve the problem. \cite{Selin2014} \\
In the following we will look at tests for the fluid solvers both alone, testing laminar to turbulent flow, and with solid. We will test the solid solver, and lastly the entire coupled FSI problem. 
\subsection{Taylor-Green vortex}
The Taylor-Green vortex problem is used to examine if our N-S code has the ability to correctly simulate vortex decay and turbulence \cite{DeBonis2013}.
\subsubsection{Problem definition}
Using a cube with sides $2\pi$. \newline
We have an initial distribution of velocity $\bar{u} = (u,v,w)$:
\begin{align}
u(x,y,z) &= V_0sin(x)cos(y)cos(z) \\
v(x,y,z) &= - V_0cos(x)sin(y)cos(z)  \\
w(x,y,z) &= 0  
\end{align}
The Reynolds number is defined as: $Re = \frac{V_0 L}{\nu}$ where we set $V_0 = 1$

\subsection{Fluid-Structure Interaction between an elastic object and laminar incompressible flow}

\input{./Verification_Validation/Hron_Turek/Hron_Turek}









